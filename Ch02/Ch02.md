
# A little history

This is a technical book, but I believe origin stories are as important as operation or design details. And so, I would like to tell you the actual story about how RavenDB began. I'm going to be talking about myself quite a lot for this section, but we'll resume talking about RavenDB immediately after, I promise.

You can skip this chapter and go directly to the technical content if you wish, but I do suggest reading it at some point.

## The back story

In September 2007, I was sitting in JAOO, listening to [Joe Armstrong](http://joearms.github.io/) talk about Erlang, distributed systems and 9 nines reliability. 
After, I bought his [Programming Erlang](http://pragprog.com/book/jaerlang2/programming-erlang) book and read it cover to cover. I thought the ideas presented in both the talk and the book were fascinating, but just _reading_ about them in a book wasn't enough. Armed with just enough knowledge about Erlang to be dangerous, I decided that I need something bigger.

At the time, [CouchDB](http://couchdb.apache.org/) was one of the most cited Erlang projects, so I decided that I would go over its code and learn how production Erlang code is written. 
I [blogged about the experience](http://ayende.com/blog/3607/reading-erlang-inspecting-couchdb), so you can read my raw thoughts at the time. And so, my first real introduction to NoSQL was actually reading through the codebase of a production quality database. It was a fascinating journey.

But I didn't (and don't) _like_ the Erlang syntax. And I disagreed with several aspects of the CouchDB approach. And that was how it stayed for quite some time. Since 2004 or so, I have been dealing primarily with Object Relational Mappers and relational databases. I'm a core team member of the NHibernate Project and I've been a consultant for much of that time.

Taken together, this means that ever since 2004, my job was largely to go to clients improve the performance, stability, and scalability of their applications. 

The problem with that was that at some point, I made a mistake. I was doing a code review for two clients, and I sent each of them the report about the issues found in the _other client's code_ and how to resolve it. Mistakes like that aren't pleasant, but they happen. So, imagine my surprise when both the clients _didn't notice that they had a report about a completely different codebase_ before I pointed it out. In fact, one client was actually fast enough to _implement_ my suggestions before I could get back to them. Although they did comment that I was probably working on top of a different branch :-).

### All the same mistakes...

That led me to go back and review all of my work for the past several years, and realize that I was really doing the exact same thing, over and over again. I was working for clients that ranged from Fortune 100 to small start-ups, and in diverse industries such as health care, social media, risk management and real estate management. But the _problems_ that the clients run into were the same. And the _solutions_ were often the same as well.

It got to the point where I would tell my clients that if I wasn't able to give them a *major* performance boost within two days of arriving, I would cut my rate by 50%. I never had to do that, because usually within two hours of arriving at a customer, I was able to point out one of two or three very common issues that would drastically improve their situation.

I was very deeply into NHibernate consulting at the time. Which meant that most of my consulting was done around database interactions or the overall system architecture. In one memorable case, I helped a customer reduce the load on his system from roughly 17,000 (seventeen thousand) queries per page to a "mere" 75 queries per page. That is correct -- they had 17 _thousand_ queries per page. In production. For several years. Admittedly, that is quite an extraordinary case. But my target was 75% - 90% reduction in the number of queries that the system made, and a comparable increase in performance.

Usually I actually got to use my NHibernate knowledge to optimize the way the system accessed the database. But that usually happened at least several days into my visit. The first few days _always_ consisted of finding the common Select N+1 issues, finding the common Unbounded Result Set issues, etc. I didn't even try to find _all_ of those issues, because they were so prevalent that just fixing the first three or five would usually give the system a very big boost in performance and make the customer happy.

It made _me_ very sad, however. I was the database consultant equivalent of a plumber. I would go to a client, "unclog" the database, give some advice about how to do better, and move on to another client to do the exact same thing. I blogged about those issues extensively, and most of the people who invited me to look over their code were readers of my blog. By definition, then, they weren't stupid, careless or malicious. In fact, most of them were dedicated, hard working and very good at their jobs.

### Maybe tooling will help?

Something had to give, because if I saw yet another Select N+1 bug, I felt that I would just scream. So in 2008 I started working on what became the NHibernate and Entity Framework Profilers. These tools monitor the interaction between your application and a relational database. Based on that information, they can show you how your code interacts with the database and what queries are being generated by what methods. On top of that, they analyze the database interactions and use that information to raise warnings and suggest improvements.

Building this tool meant that I could give it to my clients, and they would have the same insights as I did about how to properly work with a relational database. It also gave me an interesting project to work on that didn't involve finding the same issues over and over again with slight variations on the naming conventions.

The profilers are wonderful tools when you are working on relational databases, but they didn't actually solve the problems that I saw over and over again. It was not easy to pinpoint where the application was doing silly things. In essence, that meant that we had just moved the problem; no longer having to deal with the common errors, we now had to deal with a much more complex ones. How could we get all of that complex data to the user, manipulate it properly and persist it in a consistent, fast and maintainable way?

The unfortunate answer was that we really couldn't. There were quite a lot of constraints placed upon us by the choice of relational data store. Relational databases are wonderful things -- masterpieces of engineering thought and the culmination of decades of experience. They are also, quite often, the wrong tool for the job.

### Think different (database)...

It was at that point that I actually took the very daring steps of trying to set out and actually plan what would be my ideal database. The kind of database that wouldn't make me fight it. The kind of database that would allow me to really just get things done, instead of writing another schema migration script.

I thought about it often enough that I actually had a [whole design sketched out](http://ayende.com/blog/3897/designing-a-document-database) in a series of blog posts. The idea just wouldn't leave me; eventually I broke down started writing the code. Initially, I thought it would just be a spike, or another OSS project. I called it Rhino.DivanDB -- I think you can guess what inspired me.

The problem is that pronouncing Rhino.DivanDB is pretty hard (try saying it out loud several times). Eventually, I realized that I was actually calling it RivanDB. From there, it was just a matter of finding a word that was close and made sense. In the end, this is how RavenDB was born.

![Raven on rock](.\Ch02\Figure-01.png)

Since then, I've become quite fond of ravens; many of the internal components inside RavenDB are now named after various ravens. At around the same time that I decided on the name change, I also realized that it wouldn't be enough for me to undertake this as only an open source project.

I had a burning desire to actually go and _make_ this. And not just as a random Github repository -- I wanted to make it great. That meant that it had to be a product. Discussing the nuances of product vs. project is out of scope here -- even in this seemingly off-topic section -- but in general, it meant I had to sit down and formulate an actual business plan. I needed to plan what I was going to invest, how the money was going to come in, the cut off point if it was an utter failure, and at what point I'd allow myself to drink champagne. 

### Setting RavenDB free...

In May of 2010, there was a public release of RavenDB 1.0. That was quite an event, and I was very pleased with what we had managed to achieve. The plan called for working on RavenDB, showing it off, and building confidence in it. I expected this process to take 18 months, since the lead time for something as critical as a database is usually very long. Instead, we had a system running in production using RavenDB within four months! And the pick up since then has been well above what I initially expected.

Oh well, I'll settle for building great databases, rather than realistic business plans :-).

Today, the RavenDB Core Team has about 15 full time developers, and the limiting factor is the quality we require from our people. RavenDB runs mission critical systems from healthcare to banking from government institutions to real estate brokerage systems. Several books has been written about RavenDB, articles about it show up regularly in trade journals and you can hear talks in user groups and conferences across the world.

I'm quite happy with this situation, as you can imagine. And RavenDB is just getting better...

All of that said, the back story might be interesting to you or not, but you aren't here to read about the history of RavenDB. You are reading this because you want to use RavenDB in your future. And that means that you need to understand why you'll want to do that...

## The case for a non relational data store.

Edgar F. Codd formulated the relational model in 1969. Ten years later, Oracle 2.0 comes to the market. And Sybase SQL Server came out with its first version in 1984. By the early 90s, it was clear that relational database has pushed out the competition (such as navigational or object oriented databases) to the sidelines. It made sense, you could do a lot more with a relational database, and you could do it easier, usually faster and certainly in a more convenient manner.

Let us look at what environment those relational databases grew up in[^1]. In 1979, you could buy the [IBM's 3370](http://www-03.ibm.com/ibm/history/exhibits/storage/storage_3370.html) direct access storage device. It offered a stunning 571MB (read, megabytes) of storage for the mere cost of $35,100. 
For reference the yearly salary of a programmer [at that time](http://www.bls.gov/opub/mlr/1982/10/rpt2full.pdf) was $17,535. In other words, the cost of a single 571MB hard drive was as much as _two full time developers_.

[^1]: This is a somewhat apples & oranges comparisons that I'm making here. It is hard to get pricing on a consistent set of hard drives from those years. Some of the hard drive listed here would be considered enterprise grade hardware, while others are consumer grade. The intent is mostly to give you a good idea about the relative costs.

In 1980, we had the first GB drive, for merely $40,000, weighting 550 pounds and about as big as a refrigerator. By 1986, the situation [improved](http://walt.lishost.org/2010/11/musing-about-hard-disks/) and purchasing a good internal hard drive with all of 20MB at merely $800. For reference, a good car at the time would cost you less than $7,000.

Skipping ahead again, by 1996 you could actually purchase a 2.83 GB drive for merely $2,900. A car at that time would cost you $12,371. I could go on, but I'm sure that you get the point by now. Storage used to be _expensive_. So expensive that it dominated pretty much every other concern that you can think of.

At the time of this writing, you can get a 6 TB drive for less than $300 ^[WD60EFRX - if you really care]. And a 3 TB drive will cost you roughly $100. That is 2014 for roughly 30 _cents_ per gigabyte, and 1980 for 40,000 _dollars_ per gigabyte ^[Note that we are ignoring 30 years of inflation, too.].

Even leaving this aside, we also have to consider the type of applications that were written at that time. In the 80s and early 90s, the absolute height of user interface was the master/details form. And the number of users you had could usually be counted on a single finger. 

That environment produced databases that were optimized to answer the kind of challenges that prevailed at the time. Storage was _expensive_, so a major effort was made to reduce storage as much as possible. Users' time was very cheap by comparisons, so trade offs that meant that we could save some disk space at the expense of making the user wait were good design decisions. Time passed, machines got faster and cheaper, disk space became _cheap_, making the user wait became unacceptable, but we are still seeing those tradeoffs today.

Those are mostly evident when you look at normalization, fixed schemas and the relational model itself.

### Normalization, compression and data corruption

You've very likely used a relational database in the past. That means that you've learned about normalization, and how important that is. Words like data integrity are thrown around quite often, usually. But the original purpose of normalization had everything to do with reducing duplication to the maximum extent.

A common example for normalization is addresses. Instead of storing a customer's address on every order that he has, we'll simply store the address id in the order, and we have saved ourselves the need to update the address in multiple locations. You can see a sample of such a schema in Figure 3 ^[You can also access the [schema online](http://dbdsgnr.appspot.com/app#agdkYmRzZ25ycg8LEgZTY2hlbWEYqc28Bww)].

![A simple relational schema for orders](.\Ch02\Figure-03.png)

You've seen (and probably wrote) such schemas before. And at a glance, you'll probably agree that this is a reasonable way to structure a database for order management. Now, let explore what happens when the customer wishes to change his address. The way the database is set up, we can just update a single row in the Addresses table. Great, we're done.

Except... we've just introduce a subtle but deadly data corruption for our database. If that customer has existing orders, both those orders and the customer information are all pointing at the same address. Updating the address for the customer therefor will also update the address for _all of its orders_. When we'll look at one of those orders, we'll not see the address that it was shipped to, but the _current_ customer address.

In the real world, I've seen such things happen with payroll systems and paystubs (payslips across the pond). An employee got married, and changed her bank account information to the new shared bank account. The couple also wanted to purchase a home, so they applied for a mortgage. As part of that, they had to submit paystubs from the past several months. That employee requested that HR department send her the last few stubs. When the bank saw that there were paystubs made to an account that didn't even existed, they suspected fraud, the mortgage was denied and the police was called. An unpleasant situation all around.

The common response for showing this issue is that it is an issue of bad modeling decisions (and I agree). The problem is that the appropriate model would mean that each order has its own address id in the addresses table. That isn't a really good idea, you'll have to do additional joins to get the data.
Combine that with a real world model of even moderate complexity and the size and cost of the model just explodes. 

### Unclear consistency boundaries

Modeling concerns in a relational database are also complex because there is an impedance mismatch between the way we model our business objects and the way the relational database forces us to persist them.
Because a relational database is limited to tables, rows, and columns we are forced to spread a single entity across multiple tables. Let us look at the Orders table in Figure 3. An Order is an entity in our system. It has its own unique existence. However, the order lines only serve to store data about the order, they don't really have a right to exist independently.

The problem with a relational database in this case is that the consistency boundaries that it has is the row. However, in our business model, the consistency boundary isn't an order line. That isn't meaningful. The consistency boundary is the Order. More generally, the consistency boundary we have is the Aggregate Root.

That leads to a lot of contortions when using relational databases to get the kind of consistency boundary that we need. It require to use coarse grain locking, and be very careful about how we approach changing the database, lest we forget to lock the Root Aggregate and corrupt our data. And when we are talking about deep object graphs, just _getting_ to the root aggregate can be very expensive. It is expensive because relational databases are optimized for exactly the wrong thing.

### It isn't optimized for your scenarios

If your application is a typical one, you usually see a rate of a single write for every dozens or hundreds of reads. However, a relational database is heavily optimized toward writes over reads. 
It is relatively cheap to update a row (add a line item to an order), but it is much more expensive to read (need to read the order row, join to the order lines, load the products, the customer information, etc.).

Remember when relational database were designed and built. It made a lot of sense to do this optimization, because making the user wait a bit wasn't a big deal at all. The machine's time was a lot more expensive than the user's time. Today, that is a strange decision indeed, and something that many applications are suffering from.

### Change is hard, let's not do that

Probably the most frustrating one is the sheer levels of _friction_ that are involved in working with a relational database. You have to define your schema up front (usually this is the time when you know the _least_ about your project and your needs). Any change in the system require a lot more work, and there is no easy way to actually manage the current state of the database in a way that works nicely within your development cycle.

As anyone who has ever had to maintain schema change scripts, deployment to production when you have a new version is a nightmare, and just managing that in source control, so your database and your code are in sync is a non trivial task. This leads developers to get very creative with approaches for avoiding schema changes. In one memorable case, the Type of a customer was defined as an Int32. There were only 3 possible customer types, and that left 28 whole bits available for usage before they had to introduce a new column. 
Untangling that several years later was an utter joy, as you can imagine.

But all of those are just stuff that make it hard to work with relational databases. If it wasn't for the internet and the need for "web-scale" solution, we probably wouldn't be talking about NoSQL or RavenDB.

### Scale? Sure, get a bigger box

A hidden assumption in the relational model, the entire dataset is available at all time, and there is little if any difference regarding the access times to different pieces of data (ignoring memory vs disk for now). This assumption held true as long as we could use a single machine to hold the entirety of the dataset. When talking about web-scale data sets, that is far from feasible.

The usual scaling method for relational databases was to buy a bigger box. Rinse & Repeat until you run out of bigger boxes, and at that point, you pretty much stuck.

Since my day job is building databases, let us assume that I got the requirement to build a relational database that would allow distribution of data among multiple nodes. The first thing to do would be to create a table with a primary key. We can just decide that certain key ranges would go to certain nodes, and we can move on. That does raise the issue of what to do when a node is down. I will not be able to read or write any rows which fall in this node range.^[Yes, we can avoid this by duplicating the data on multiple nodes, but that just moves the problem around, instead of one node being down, we need two or three nodes down to have the same effect.]

We'll ignore this problem for now and try to implement the next feature, a unique constraint. That is required so I can't have multiple users with the same email. But this is just making things that much harder again. Now every insert or update to a user's email will require us to talk to the other nodes. And what happens if one of the nodes is unavailable?  At this point, I don't _know_ if I have this email or not. I might have, if it is located in the node that I cannot access.

We'll ignore this problem as well, and just assume that we have competent and awesome DBAs and that nodes can never go down. What is the cost of making a query? Well, simple queries I can probably route to a single node. But complex queries?

Consider the following query, using the schema we have seen in Figure 3.

```sql
SELECT * FROM Orders o
	JOIN OrderLines ol 	on ol.OrderId = o.Id
	JOIN Products p 	on ol.ProductId = p.Id
	JOIN Addresses a 	on o.AddressId = a.Id
	JOIN Customers c 	on o.CustomerId = c.Id
WHERE o.Id = 7331
```

In a system with multiple nodes, how costly do you think this query is going to be? This is a pretty simple query, but it still likely to require us to touch multiple nodes. Even ignoring the option for failure, this is a ridiculously expensive way to do business. And the more data you have, the more nodes you have, the more expensive this becomes.

As the need for actual web-scale grew, people have noticed that it is not really possible to scale out a relational database. And there is only so much scaling up that you can do. This is where the need for an alternative solution came into play. Thus the need for NoSQL.

## Making sense in the NoSQL menagerie

One of the hardest things about NoSQL is that it is defined in a negative term. The Windows Registry is a NoSQL database, for example. It is a database, and it doesn't use either SQL or a relational model.
In general, NoSQL databases are non relational, can scale out more easily and are designed from the ground up for the modern operating environment.

Typically, one count four different types of NoSQL databases:

* Key/Value Databases 
* Graph Databases
* Column Family Databases
* Document Databases

### Key/Value databases... 

Database systems that store a key and its associated value, as the name might have told you. Some of them allow operations on the value (such as defining a value as a set or a list that can be manipulated server side), but most of the time, you are just working with the plain raw value (which is a byte array).

The good thing about them is that it is pretty easy to scale them, because they offer a very simple API and they tend to be quite fast, since their internal works is pretty simple. Scaling out is a matter of adding more servers and updating the way we split the data among the servers^[_Highly_ simplified view, I'm aware, but good enough for our purposes right now.]. The bad thing about them is that they are very simple, so a lot of the onus of using them properly is on the user. You cannot make queries using a key/value store, you have to maintain a secondary index manually, for example.

### Graph databases...

Composed of nodes and edges, graph databases allow you to model your domain using graphs. That can be very helpful when you want to deal with highly connected data, such as social graphs, travel options or network flows. Such databases are very useful when one of the most important aspects of the data isn't the just the data itself, but the association between different pieces of information.

Graph databases allow you to define nodes and associate those using edges, and then allow queries to traverse them as needed. Some databases also offer some graph algorithms (shortest path, Dijkstra's, etc.), but a lot of them function mostly as a queryable adjacency list.

A major flaw is scaling such systems is that most graphs tend to be highly inter connected, and it is very hard to isolate an independent sub graph and break it into a separate machine. Consider the classic six degrees of separation theory, and that the average distance between any two random Twitter user is [less than four](http://en.wikipedia.org/wiki/Six_degrees_of_separation#An_optimal_algorithm_to_calculate_degrees_of_separation_in_social_networks).

Because of that, the use of graph databases is usually limited to just the associations that need to be handled. Most of the actual data is stored in another type of database.

### Column databases...

A column database (sometimes called column family database) are meant to store very large datasets. The data is structured as rows, but unlike relational databases, where the row schema is fixed, the columns are sparse and you can have as many of them as you want. It is typical to have different rows with different columns, and the column names aren't just metadata, they are part of the data.

The actual storage of the data is done on a sparse column basis. Which means that aggregation of the data is very quick. For the most part, that means that you need to pay attention to the actual structure of the data as much as to the data itself. Column databases are good for very big datasets that need to be distributed over many nodes. They have a high degree of operational and development complexity, and offer little advantages if your dataset is small enough to fit on a small number of nodes.

### Document databases...

Document databases are very similar to key/value stores. They store a document by key. But unlike a key/value store, where the value is usually arbitrary, in a document database the format^[But not the schema] of the documents is well known, usually JSON. That means that the database server can perform operations on that data.

Those operations include querying, partial document updates and aggregation (among others). The major benefits of document databases are easy model to work with, smooth scaling model and good support from the database engines. In particular, document databases are very well suited for OLTP^[Online Transaction Processing] and Domain Driven Design.

Since OLTP application is what I did for a living for a long time, it is not a surprise that this is the area where RavenDB is focused.

## 2nd generation document database

A guy wakes up one day, and decides he want to build a database...

No, you don't get to hear the rest of the joke. But this is a serious question. Why build another NoSQL database? When sitting down to design what would become RavenDB, I had a few goals in mind. 
At the time, I mostly dealt with consulting to clients building line of business applications. Those ranged from small potatoes such as "we need to track expenses" to medium size risk management system to monster systems running the core business processes for Fortune 100 companies.

The core principal for most line of business applications is OLTP, which stands for OnLine Transaction Processing. That is the major area that I set out to building RavenDB to serve. RavenDB success means that it would be _the_ obvious choice for any OLTP system. And I think we have gone quite a bit toward that destination.

RavenDB design goals included:

* Zero friction throughout (dev & ops)
* Zero admin
* Safe by Default
* Transactional / ACID
* Easily scalable

Note that most of those design goals are actually just different ways to say the same thing. Basically, the goal of RavenDB is that it Get Out of Your Way and Just Works.

### Let me do my job, and you'll do yours

We had to do quite a lot of work in order to achieve the It Just Works model. The RavenDB engine will monitor itself and (within configured limits) knows how to auto tune itself on the fly. The Safe by Default design means that it is harder (sadly, it is still not impossible) to shoot yourself in the foot.

But probably most importantly, we have look at other databases, relational and NoSQL alike, and figured out what the pain points were. What we found was that mostly, people were struggling because it was so _hard_. In order to deploy a NoSQL solution you had to become an expert on your database, and even then, you usually had to babysit it quite often. I'm talking about everything from installation and configuration, to looking at the data your put inside the database, to understanding errors and monitoring the system in production.

That was doubly true when you consider that my usual ecosystem is the .NET framework. That meant that most of my applications and systems are actually running on Windows. And most NoSQL solutions either flat out couldn't run, or could run, but only as alpha quality software. My goal was to create a really good database that .NET developers could use. As it turned out, we managed to do quite a bit more than that. 

As a small example of that. Here are the installation instructions for RavenDB:

* Go to the [RavenDB Download](http://ravendb.net/download) page.
* Download the latest stable build
* If you've downloaded the zip archive, unzip it and double click Start.cmd
* Alternatively, if you've downloaded the MSI file, install it and follow the wizard.

You now have a running RavenDB database, and you can browse to (by default) http://localhost:8080/ to access the RavenDB Studio. The point in these instructions isn't so much to explain how to install RavenDB, we'll touch on that later. The point is that this is literally the smallest number of steps that we could get to getting RavenDB up & running.

This is probably a good time to note that this is actually how _we_ deploy to our own production environment. As part of our dogfooding effort, we always run our systems using the default configuration, to make sure that RavenDB can optimize itself for our needs automatically.

Most people get excited when they see that RavenDB ships with a fully functional management studio. There is no additional tool to install, just browse to the database URL and you can start working with your data. 
To be honest, even though we invested a lot of time and effort in the studio, that is quite insulting. We've spent even more time making sure that the actual database engine is pretty awesome, and people get hung up on the UI.

A picture is worth a thousand words, and I think that Figure 2 can probably help you understand what we _didn't_ want to have.

![1st gen NoSQL](.\Ch02\Figure-02.png)

When I initially started looking at the NoSQL landscape, there were a _lot_ of really great ideas, and some projects that really picked up traction. But all of them were focused on solving the technical details, let us create a database that can do this and that. This resulted in expert tools. The kind that you could do really great things with, but only if you _were_ an expert. If you weren't an expert, however, those kind of tools would be worse than useless. You might end up removing more than just your foot, trying to use them.

The inspiration behind RavenDB was simple. It doesn't have to be so hard. RavenDB was conceived and designed primarily so we can take the essence behind the NoSQL movement and translate that to the kind of tooling that you can use even without spending three months of your life learning the ins and outs of your tooling. This was done, ruthlessly finding all points of friction and eliminating them with extreme prejudice.

Being able to see all the hurdles that other projects had to deal with meant that we were able to avoid most of them and provide a consistent, simple and pleasant experience for our users. That was five years ago, at the time of this writing. Since then, we have continue to push things forward.

The purpose of this book is to provide you with all the information you need to develop, deploy and manage a RavenDB based application in your own organization. This completes the side track into history and my reasons into starting to develop RavenDB, now we can move on to the real juicy stuff. Actually using RavenDB... enjoy the ride.
